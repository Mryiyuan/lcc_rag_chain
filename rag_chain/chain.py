from openai import OpenAI
from config import Config
from vector_db.chroma_db import get_retriever
from utils.helpers import format_docs
import re
from openai import OpenAI


def run_rag_chain(query):
    """Processes a query using a Retrieval-Augmented Generation (RAG) chain.

    This function utilizes a RAG chain to answer a given query. It retrieves 
    relevant context using similarity search and then generates a response 
    based on this context using a chat model. The chat model is pre-configured 
    with a prompt template specialized in pharmaceutical sciences.

    Args:
        query (str): The user's question that needs to be answered.

    Returns:
        str: A response generated by the chat model, based on the retrieved context."""
    # Create a Retriever Object and apply Similarity Search
    retriever = get_retriever()
    
    # Get relevant documents
    docs = retriever.invoke(query)
    context = format_docs(docs)

    # Initialize OpenAI client for vLLM
    client = OpenAI(
        base_url=Config.VLLM_API_BASE,
        api_key=Config.VLLM_API_KEY
    )

    # Create prompt with context
    prompt = f"""You are a highly knowledgeable assistant. 
Answer the question based only on the following context:
{context}

Answer the question based on the above context:
{query}

Use the provided context to answer the user's question accurately and concisely.
Don't justify your answers.
Don't give information not mentioned in the CONTEXT INFORMATION.
Do not say "according to the context" or "mentioned in the context" or similar."""

    # 处理无思考模式
    if Config.NO_THINK_MODE:
        prompt = "/no_think\n" + prompt

    try:
        # Call vLLM API
        response = client.chat.completions.create(
            model=Config.VLLM_MODEL_NAME,
            messages=[
                {"role": "user", "content": prompt}
            ],
            max_tokens=Config.MAX_TOKENS,
            temperature=Config.TEMPERATURE
        )
        
        # 剔除思考区块
        assistant_response = response.choices[0].message.content or ""
        assistant_response = re.sub(r'</think>.*?</think>', '', assistant_response, flags=re.S).strip()
        return assistant_response
    except Exception as e:
        return f"Error generating response: {str(e)}"


def run_rag_chain_stream(query, chat_history=None):
    """Processes a query using a Retrieval-Augmented Generation (RAG) chain with streaming output.

    This function utilizes a RAG chain to answer a given query. It retrieves 
    relevant context using similarity search and then generates a response 
    based on this context using a chat model. The response is streamed token by token.

    Args:
        query (str): The user's question that needs to be answered.
        chat_history (list, optional): List of previous messages in the conversation.

    Yields:
        str: Chunks of response generated by the chat model."""
    # Create a Retriever Object and apply Similarity Search
    retriever = get_retriever()
    
    # Get relevant documents
    docs = retriever.invoke(query)
    context = format_docs(docs)

    # Initialize OpenAI client for vLLM
    client = OpenAI(
        base_url=Config.VLLM_API_BASE,
        api_key=Config.VLLM_API_KEY
    )

    try:
        # 构建包含聊天历史的消息列表
        messages = []
        
        # 添加系统消息（角色设定）
        system_content = "You are a highly knowledgeable assistant."
        messages.append({"role": "system", "content": system_content})
        
        # 添加聊天历史
        if chat_history:
            for msg in chat_history:
                if isinstance(msg, dict) and "role" in msg and "content" in msg:
                    messages.append(msg)
        
        # 根据是否有上下文调整用户提示
        if context and context.strip():
            # 如果有上下文，使用RAG模式
            user_prompt = f"""Answer the question based on the following context if relevant:
{context}

Question: {query}

If the context provides relevant information, use it to answer accurately. If not, you can use your own knowledge."""
        else:
            # 如果没有上下文，使用普通对话模式
            user_prompt = query
        
        # 处理无思考模式
        no_think_prefix = "/no_think\n" if Config.NO_THINK_MODE else ""
        user_prompt = no_think_prefix + user_prompt
        
        # 添加当前查询
        messages.append({"role": "user", "content": user_prompt})
        
        # Call vLLM API with streaming
        response = client.chat.completions.create(
            model=Config.VLLM_MODEL_NAME,
            messages=messages,
            max_tokens=Config.MAX_TOKENS,
            temperature=Config.TEMPERATURE,
            stream=True
        )
        
        # 流式处理响应 - 先收集完整响应，然后一次性清理并输出
        # 这样可以避免使用退格字符导致的乱码问题
        full_response = ""
        for chunk in response:
            if chunk.choices[0].delta.content is not None:
                delta_content = chunk.choices[0].delta.content
                full_response += delta_content
                # 实时流式输出原始内容
                yield delta_content
        
        # 注意：如果需要清理思考区块，应该在收集完整响应后一次性处理
        # 但为了保持流式输出的连续性，我们选择直接流式输出原始内容
    except Exception as e:
        yield f"Error generating response: {str(e)}"