from config import Config
from vector_db.chroma_db import get_retriever
from utils.helpers import format_docs
from utils.reranker import rerank_documents
import re
from openai import OpenAI


def run_rag_chain(query):
    """Processes a query using a Retrieval-Augmented Generation (RAG) chain.

    This function utilizes a RAG chain to answer a given query. It retrieves 
    relevant context using similarity search and then generates a response 
    based on this context using a chat model. The chat model is pre-configured 
    with a prompt template specialized in pharmaceutical sciences.

    Args:
        query (str): The user's question that needs to be answered.

    Returns:
        str: A response generated by the chat model, based on the retrieved context."""
    # Create a Retriever Object and apply Similarity Search
    retriever = get_retriever()
    
    # Get relevant documents
    docs = retriever.invoke(query)
    
    # Apply reranking if enabled
    if Config.RERANK_ENABLED:
        docs = rerank_documents(query, docs)
    
    context = format_docs(docs)

    # Initialize OpenAI client for vLLM
    client = OpenAI(
        base_url=Config.VLLM_API_BASE,
        api_key=Config.VLLM_API_KEY
    )

    # Create prompt with context
    prompt = f"""You are a highly knowledgeable assistant. 
Answer the question based only on the following context:
{context}

Answer the question based on the above context:
{query}

Use the provided context to answer the user's question accurately and concisely.
Don't justify your answers.
Don't give information not mentioned in the CONTEXT INFORMATION.
Do not say "according to the context" or "mentioned in the context" or similar."""

    # Handle no-think mode
    if Config.NO_THINK_MODE:
        prompt = "/no_think\n" + prompt

    try:
        # Call vLLM API
        response = client.chat.completions.create(
            model=Config.VLLM_MODEL_NAME,
            messages=[
                {"role": "user", "content": prompt}
            ],
            max_tokens=Config.MAX_TOKENS,
            temperature=Config.TEMPERATURE
        )
        
        # Remove thought blocks
        assistant_response = response.choices[0].message.content or ""
        assistant_response = re.sub(r'</think>.*?</think>', '', assistant_response, flags=re.S).strip()
        return assistant_response
    except Exception as e:
        return f"Error generating response: {str(e)}"


def run_rag_chain_stream(query, chat_history=None):
    """Processes a query using a Retrieval-Augmented Generation (RAG) chain with streaming output.

    This function utilizes a RAG chain to answer a given query. It retrieves 
    relevant context using similarity search and then generates a response 
    based on this context using a chat model. The response is streamed token by token.

    Args:
        query (str): The user's question that needs to be answered.
        chat_history (list, optional): List of previous messages in the conversation.

    Yields:
        str: Chunks of response generated by the chat model."""
    # Create a Retriever Object and apply Similarity Search
    retriever = get_retriever()
    
    # Get relevant documents
    docs = retriever.invoke(query)
    
    # Apply reranking if enabled
    if Config.RERANK_ENABLED:
        docs = rerank_documents(query, docs)
    
    context = format_docs(docs)

    # Initialize OpenAI client for vLLM
    client = OpenAI(
        base_url=Config.VLLM_API_BASE,
        api_key=Config.VLLM_API_KEY
    )

    try:
        # Build message list with chat history
        messages = []
        
        # Add system message (role setting)
        system_content = "You are a highly knowledgeable assistant."
        messages.append({"role": "system", "content": system_content})
        
        # Add chat history
        if chat_history:
            for msg in chat_history:
                if isinstance(msg, dict) and "role" in msg and "content" in msg:
                    messages.append(msg)
        
        # Adjust user prompt based on whether there's context
        if context and context.strip():
            # If there's context, use RAG mode
            user_prompt = f"""Answer the question based on the following context if relevant:
{context}

Question: {query}

If the context provides relevant information, use it to answer accurately. If not, you can use your own knowledge."""
        else:
            # If there's no context, use normal conversation mode
            user_prompt = query
        
        # Handle no-think mode
        no_think_prefix = "/no_think\n" if Config.NO_THINK_MODE else ""
        user_prompt = no_think_prefix + user_prompt
        
        # Add current query
        messages.append({"role": "user", "content": user_prompt})
        
        # Call vLLM API with streaming
        response = client.chat.completions.create(
            model=Config.VLLM_MODEL_NAME,
            messages=messages,
            max_tokens=Config.MAX_TOKENS,
            temperature=Config.TEMPERATURE,
            stream=True
        )
        
        # Process streaming response - first collect full response, then clean and output
        # This avoids garbled characters caused by backspace characters
        full_response = ""
        for chunk in response:
            if chunk.choices[0].delta.content is not None:
                delta_content = chunk.choices[0].delta.content
                full_response += delta_content
                # Stream output raw content in real-time
                yield delta_content
        
        # Note: If thought block cleaning is needed, it should be done after collecting full response
        # But to maintain the continuity of streaming output, we choose to stream output raw content directly
    except Exception as e:
        yield f"Error generating response: {str(e)}"